{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcwdLW0M0KRM",
        "outputId": "4fe4a8cd-6edd-4ce0-d514-910699f90ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.12.14)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install ucimlrepo\n",
        "! pip install numpy pandas scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Лабораторная работа 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Giiy5l_0HCz",
        "outputId": "52e6a344-5033-4892-abe2-e0b6d33ce58f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[INFO] Breast Cancer dataset loaded.\n",
            "Features shape: (569, 30), Target shape: (569,)\n",
            "\n",
            "[INFO] Concrete dataset metadata:\n",
            "{'uci_id': 165, 'name': 'Concrete Compressive Strength', 'repository_url': 'https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength', 'data_url': 'https://archive.ics.uci.edu/static/public/165/data.csv', 'abstract': 'Concrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. ', 'area': 'Physics and Chemistry', 'tasks': ['Regression'], 'characteristics': ['Multivariate'], 'num_instances': 1030, 'num_features': 8, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['Concrete compressive strength'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1998, 'last_updated': 'Sun Feb 11 2024', 'dataset_doi': '10.24432/C5PK67', 'creators': ['I-Cheng Yeh'], 'intro_paper': {'ID': 383, 'type': 'NATIVE', 'title': 'Modeling of strength of high-performance concrete using artificial neural networks', 'authors': 'I. Yeh', 'venue': 'Cement and Concrete Research, Vol. 28, No. 12', 'year': 1998, 'journal': None, 'DOI': '10.1016/S0008-8846(98)00165-3', 'URL': 'https://www.semanticscholar.org/paper/9310cae70452ea11465f338483e79cc36a68881c', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'Number of instances \\t1030\\r\\nNumber of Attributes\\t9\\r\\nAttribute breakdown\\t8 quantitative input variables, and 1 quantitative output variable\\r\\nMissing Attribute Values\\tNone \\r\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Given are the variable name, variable type, the measurement unit and a brief description. The concrete compressive strength is the regression problem. The order of this listing corresponds to the order of numerals along the rows of the database. \\r\\n\\r\\nName -- Data Type -- Measurement -- Description\\r\\n\\r\\nCement (component 1) -- quantitative -- kg in a m3 mixture -- Input Variable\\r\\nBlast Furnace Slag (component 2) -- quantitative -- kg in a m3 mixture -- Input Variable\\r\\nFly Ash (component 3) -- quantitative  -- kg in a m3 mixture -- Input Variable\\r\\nWater  (component 4) -- quantitative  -- kg in a m3 mixture -- Input Variable\\r\\nSuperplasticizer (component 5) -- quantitative -- kg in a m3 mixture -- Input Variable\\r\\nCoarse Aggregate  (component 6) -- quantitative -- kg in a m3 mixture -- Input Variable\\r\\nFine Aggregate (component 7)\\t -- quantitative  -- kg in a m3 mixture -- Input Variable\\r\\nAge -- quantitative  -- Day (1~365) -- Input Variable\\r\\nConcrete compressive strength -- quantitative -- MPa -- Output Variable\\r\\n\\r\\n', 'citation': None}}\n",
            "\n",
            "=== КЛАССИФИКАЦИЯ (Breast Cancer) ===\n",
            "\n",
            "[Бейзлайн: KNN Classifier]\n",
            "Accuracy:  0.9123\n",
            "Precision: 0.9032\n",
            "Recall:    0.9107\n",
            "F1-score:  0.9066\n",
            "\n",
            "=== РЕГРЕССИЯ (Concrete) ===\n",
            "\n",
            "[Бейзлайн: KNN Regressor]\n",
            "MSE:  68.7935\n",
            "R^2:  0.7330\n",
            "\n",
            "Лучшие параметры (Breast Cancer Classification): {'knn__metric': 'manhattan', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'scaler': MinMaxScaler()}\n",
            "[Улучшенный KNN Classifier] на тесте:\n",
            "Accuracy: 0.9737, F1-macro: 0.9713\n",
            "\n",
            "Лучшие параметры (Concrete Regression): {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'distance', 'scaler': StandardScaler()}\n",
            "[Улучшенный KNN Regressor] на тесте:\n",
            "MSE: 57.6196, R^2: 0.7764\n",
            "\n",
            "=== Custom KNN Classifier ===\n",
            "[Custom KNN Classifier] Accuracy: 0.9123, F1-macro: 0.9066\n",
            "\n",
            "=== Custom KNN Regressor ===\n",
            "[Custom KNN Regressor] MSE: 68.8046, R^2: 0.7330\n",
            "\n",
            "=== Custom KNN Classifier (Improved) ===\n",
            "[Custom KNN Classifier Improved] Accuracy: 0.9737, F1-macro: 0.9713\n",
            "\n",
            "=== Custom KNN Regressor (Improved) ===\n",
            "[Custom KNN Regressor Improved] MSE: 78.8591, R^2: 0.6940\n"
          ]
        }
      ],
      "source": [
        "##############################\n",
        "# 1. УСТАНОВКА/ИМПОРТ БИБЛИОТЕК\n",
        "##############################\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, mean_squared_error, mean_absolute_error, r2_score)\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Для загрузки Breast Cancer Wisconsin Dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Для загрузки Concrete Dataset\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "##############################\n",
        "# 1. ВЫБОР НАЧАЛЬНЫХ УСЛОВИЙ\n",
        "##############################\n",
        "\n",
        "# 1.a Breast Cancer Wisconsin Dataset (задача классификации)\n",
        "data_bc = load_breast_cancer()\n",
        "X_class = pd.DataFrame(data_bc.data, columns=data_bc.feature_names)\n",
        "y_class = pd.Series(data_bc.target)\n",
        "print(\"\\n[INFO] Breast Cancer dataset loaded.\")\n",
        "print(f\"Features shape: {X_class.shape}, Target shape: {y_class.shape}\")\n",
        "\n",
        "# 1.b Concrete Compressive Strength Dataset (задача регрессии)\n",
        "concrete_compressive_strength = fetch_ucirepo(id=165)  # id=165 для Concrete\n",
        "X_concrete = concrete_compressive_strength.data.features\n",
        "y_concrete = concrete_compressive_strength.data.targets\n",
        "\n",
        "print(\"\\n[INFO] Concrete dataset metadata:\")\n",
        "print(concrete_compressive_strength.metadata)\n",
        "\n",
        "##############################\n",
        "# 2. СОЗДАНИЕ БЕЙЗЛАЙНА\n",
        "##############################\n",
        "\n",
        "# 2.1 КЛАССИФИКАЦИЯ (Breast Cancer Wisconsin)\n",
        "\n",
        "print(\"\\n=== КЛАССИФИКАЦИЯ (Breast Cancer) ===\")\n",
        "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
        "    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        ")\n",
        "\n",
        "# Бейзлайн KNN Classifier (без тюнинга)\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_classifier.fit(Xc_train, yc_train)\n",
        "yc_pred = knn_classifier.predict(Xc_test)\n",
        "\n",
        "acc_base = accuracy_score(yc_test, yc_pred)\n",
        "prec_base = precision_score(yc_test, yc_pred, average='macro')\n",
        "rec_base = recall_score(yc_test, yc_pred, average='macro')\n",
        "f1_base = f1_score(yc_test, yc_pred, average='macro')\n",
        "\n",
        "print(\"\\n[Бейзлайн: KNN Classifier]\")\n",
        "print(f\"Accuracy:  {acc_base:.4f}\")\n",
        "print(f\"Precision: {prec_base:.4f}\")\n",
        "print(f\"Recall:    {rec_base:.4f}\")\n",
        "print(f\"F1-score:  {f1_base:.4f}\")\n",
        "\n",
        "# 2.2 РЕГРЕССИЯ (Concrete)\n",
        "\n",
        "print(\"\\n=== РЕГРЕССИЯ (Concrete) ===\")\n",
        "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
        "    X_concrete, y_concrete, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_regressor.fit(Xr_train, yr_train)\n",
        "yr_pred = knn_regressor.predict(Xr_test)\n",
        "\n",
        "mse_base = mean_squared_error(yr_test, yr_pred)\n",
        "r2_base = r2_score(yr_test, yr_pred)\n",
        "\n",
        "print(\"\\n[Бейзлайн: KNN Regressor]\")\n",
        "print(f\"MSE:  {mse_base:.4f}\")\n",
        "print(f\"R^2:  {r2_base:.4f}\")\n",
        "\n",
        "##############################\n",
        "# 3. УЛУЧШЕНИЕ БЕЙЗЛАЙНА\n",
        "##############################\n",
        "\n",
        "# GridSearchCV для классификации\n",
        "pipeline_class = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "param_grid_class = {\n",
        "    'scaler': [StandardScaler(), MinMaxScaler(), RobustScaler(), None],\n",
        "    'knn__n_neighbors': [3, 5, 7, 9],\n",
        "    'knn__weights': ['uniform', 'distance'],\n",
        "    'knn__metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "grid_search_class = GridSearchCV(\n",
        "    pipeline_class,\n",
        "    param_grid_class,\n",
        "    cv=5,\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search_class.fit(Xc_train, yc_train)\n",
        "\n",
        "print(\"\\nЛучшие параметры (Breast Cancer Classification):\", grid_search_class.best_params_)\n",
        "best_clf = grid_search_class.best_estimator_\n",
        "\n",
        "yc_pred_best = best_clf.predict(Xc_test)\n",
        "acc_best = accuracy_score(yc_test, yc_pred_best)\n",
        "f1_best = f1_score(yc_test, yc_pred_best, average='macro')\n",
        "\n",
        "print(\"[Улучшенный KNN Classifier] на тесте:\")\n",
        "print(f\"Accuracy: {acc_best:.4f}, F1-macro: {f1_best:.4f}\")\n",
        "\n",
        "# GridSearchCV для регрессии\n",
        "pipeline_reg = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('knn', KNeighborsRegressor())\n",
        "])\n",
        "\n",
        "param_grid_reg = {\n",
        "    'scaler': [StandardScaler(), MinMaxScaler(), None],\n",
        "    'knn__n_neighbors': [3, 5, 7, 9],\n",
        "    'knn__weights': ['uniform', 'distance'],\n",
        "    'knn__metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "grid_search_reg = GridSearchCV(\n",
        "    pipeline_reg,\n",
        "    param_grid_reg,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search_reg.fit(Xr_train, yr_train)\n",
        "\n",
        "print(\"\\nЛучшие параметры (Concrete Regression):\", grid_search_reg.best_params_)\n",
        "best_reg = grid_search_reg.best_estimator_\n",
        "\n",
        "yr_pred_best = best_reg.predict(Xr_test)\n",
        "mse_best = mean_squared_error(yr_test, yr_pred_best)\n",
        "r2_best = r2_score(yr_test, yr_pred_best)\n",
        "\n",
        "print(\"[Улучшенный KNN Regressor] на тесте:\")\n",
        "print(f\"MSE: {mse_best:.4f}, R^2: {r2_best:.4f}\")\n",
        "\n",
        "##############################\n",
        "# 4. ИМПЛЕМЕНТАЦИЯ KNN “С НУЛЯ”\n",
        "##############################\n",
        "\n",
        "class CustomKNNClassifier:\n",
        "    def __init__(self, n_neighbors=5, metric='euclidean'):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.metric = metric\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = np.array(X)\n",
        "        self.y_train = np.array(y)\n",
        "        return self\n",
        "\n",
        "    def _distance(self, x1, x2):\n",
        "        if self.metric == 'euclidean':\n",
        "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "        elif self.metric == 'manhattan':\n",
        "            return np.sum(np.abs(x1 - x2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown metric.\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = []\n",
        "        X = np.array(X)\n",
        "        for x in X:\n",
        "            distances = [self._distance(x, x_train) for x_train in self.X_train]\n",
        "            neighbors_idx = np.argsort(distances)[:self.n_neighbors]\n",
        "            neighbors_labels = self.y_train[neighbors_idx]\n",
        "            values, counts = np.unique(neighbors_labels, return_counts=True)\n",
        "            preds.append(values[np.argmax(counts)])\n",
        "        return np.array(preds)\n",
        "\n",
        "\n",
        "class CustomKNNRegressor:\n",
        "    def __init__(self, n_neighbors=5, metric='euclidean'):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.metric = metric\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = np.array(X)\n",
        "        self.y_train = np.array(y)\n",
        "        return self\n",
        "\n",
        "    def _distance(self, x1, x2):\n",
        "        if self.metric == 'euclidean':\n",
        "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "        elif self.metric == 'manhattan':\n",
        "            return np.sum(np.abs(x1 - x2))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown metric.\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = []\n",
        "        X = np.array(X)\n",
        "        for x in X:\n",
        "            distances = [self._distance(x, x_train) for x_train in self.X_train]\n",
        "            neighbors_idx = np.argsort(distances)[:self.n_neighbors]\n",
        "            neighbors_values = self.y_train[neighbors_idx]\n",
        "            preds.append(np.mean(neighbors_values))\n",
        "        return np.array(preds)\n",
        "\n",
        "# 4.a Кастомная модель (классификация)\n",
        "print(\"\\n=== Custom KNN Classifier ===\")\n",
        "custom_clf = CustomKNNClassifier(n_neighbors=5, metric='euclidean')\n",
        "custom_clf.fit(Xc_train, yc_train)\n",
        "yc_pred_custom = custom_clf.predict(Xc_test)\n",
        "\n",
        "acc_custom = accuracy_score(yc_test, yc_pred_custom)\n",
        "f1_custom = f1_score(yc_test, yc_pred_custom, average='macro')\n",
        "print(f\"[Custom KNN Classifier] Accuracy: {acc_custom:.4f}, F1-macro: {f1_custom:.4f}\")\n",
        "\n",
        "# 4.b Кастомная модель (регрессия)\n",
        "print(\"\\n=== Custom KNN Regressor ===\")\n",
        "custom_reg = CustomKNNRegressor(n_neighbors=5, metric='euclidean')\n",
        "custom_reg.fit(Xr_train, yr_train)\n",
        "yr_pred_custom = custom_reg.predict(Xr_test)\n",
        "\n",
        "mse_custom = mean_squared_error(yr_test, yr_pred_custom)\n",
        "r2_custom = r2_score(yr_test, yr_pred_custom)\n",
        "print(f\"[Custom KNN Regressor] MSE: {mse_custom:.4f}, R^2: {r2_custom:.4f}\")\n",
        "\n",
        "##############################\n",
        "# 4.c Улучшение кастомной модели\n",
        "##############################\n",
        "\n",
        "# Классификация с улучшенными параметрами\n",
        "print(\"\\n=== Custom KNN Classifier (Improved) ===\")\n",
        "custom_clf_improved = CustomKNNClassifier(n_neighbors=7, metric='manhattan')\n",
        "scaler_class = StandardScaler()\n",
        "Xc_train_scaled = scaler_class.fit_transform(Xc_train)\n",
        "Xc_test_scaled = scaler_class.transform(Xc_test)\n",
        "\n",
        "custom_clf_improved.fit(Xc_train_scaled, yc_train)\n",
        "yc_pred_cust_imp = custom_clf_improved.predict(Xc_test_scaled)\n",
        "\n",
        "acc_cust_imp = accuracy_score(yc_test, yc_pred_cust_imp)\n",
        "f1_cust_imp = f1_score(yc_test, yc_pred_cust_imp, average='macro')\n",
        "print(f\"[Custom KNN Classifier Improved] Accuracy: {acc_cust_imp:.4f}, F1-macro: {f1_cust_imp:.4f}\")\n",
        "\n",
        "# Регрессия с улучшенными параметрами\n",
        "print(\"\\n=== Custom KNN Regressor (Improved) ===\")\n",
        "custom_reg_improved = CustomKNNRegressor(n_neighbors=7, metric='manhattan')\n",
        "scaler_reg = StandardScaler()\n",
        "Xr_train_scaled = scaler_reg.fit_transform(Xr_train)\n",
        "Xr_test_scaled = scaler_reg.transform(Xr_test)\n",
        "\n",
        "custom_reg_improved.fit(Xr_train_scaled, yr_train)\n",
        "yr_pred_cust_imp = custom_reg_improved.predict(Xr_test_scaled)\n",
        "\n",
        "mse_cust_imp = mean_squared_error(yr_test, yr_pred_cust_imp)\n",
        "r2_cust_imp = r2_score(yr_test, yr_pred_cust_imp)\n",
        "print(f\"[Custom KNN Regressor Improved] MSE: {mse_cust_imp:.4f}, R^2: {r2_cust_imp:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVmVafEON_-J"
      },
      "source": [
        "Выводы:\n",
        "\n",
        "1. Бейзлайн модели (KNeighborsClassifier и KNeighborsRegressor из sklearn) показали хорошие показатели качества на обоих датасетах.\n",
        "\n",
        "2. Улучшение моделей путём гиперпараметрической настройки с использованием GridSearchCV позволило значительно повысить качество моделей как для классификации, так и для регрессии.\n",
        "\n",
        "3. Собственные реализации KNN (CustomKNNClassifier и CustomKNNRegressor) продемонстрировали основную логику алгоритма K-Nearest Neighbors, сопоставимую с базовыми моделями sklearn.\n",
        "\n",
        "3. Применение улучшений к собственным моделям оказало разное влияние на классификацию и регрессию: Разрыв в качестве между собственными моделями и моделями из sklearn по-прежнему остаётся значительным, особенно в задаче регрессии.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Лабораторная работа 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmDLKixL1Cqo",
        "outputId": "aade50ae-6376-4a6c-d7fb-e3088b6a09f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Бейзлайн (Logistic Regression) - Classification:\n",
            "Accuracy:  0.9825\n",
            "Precision: 0.9861\n",
            "Recall:    0.9861\n",
            "F1-score:  0.9861\n",
            "\n",
            "Бейзлайн (Linear Regression) - Regression:\n",
            "MSE:  95.9709\n",
            "R^2:  0.6276\n",
            "\n",
            "Улучшенная модель (Ridge + полиномиальные признаки) - Regression:\n",
            "MSE:  55.1993\n",
            "R^2:  0.7858\n",
            "\n",
            "Улучшенная модель (Logistic Regression + GridSearchCV) - Classification:\n",
            "Accuracy:  0.9737\n",
            "Precision: 0.9726\n",
            "Recall:    0.9861\n",
            "F1-score:  0.9793\n",
            "\n",
            "Custom Linear Regression - Regression:\n",
            "MSE:  89.0741\n",
            "R^2:  0.6543\n",
            "\n",
            "Custom Logistic Regression - Classification:\n",
            "Accuracy:  0.9737\n",
            "Precision: 0.9859\n",
            "Recall:    0.9722\n",
            "F1-score:  0.9790\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    mean_squared_error, mean_absolute_error, r2_score\n",
        ")\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# 1. Загрузка данных\n",
        "# Датасет для классификации (Breast Cancer Wisconsin Dataset)\n",
        "cancer_data = load_breast_cancer()\n",
        "X_class = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
        "y_class = pd.Series(cancer_data.target)\n",
        "\n",
        "# Датасет для регрессии (Concrete Compressive Strength)\n",
        "concrete_data = fetch_ucirepo(id=165)\n",
        "X_reg = concrete_data.data.features\n",
        "y_reg = concrete_data.data.targets.squeeze()\n",
        "\n",
        "# Удаление пропусков и ±∞ значений\n",
        "X_class = X_class.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "X_reg = X_reg.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "y_class = y_class.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "y_reg = y_reg.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "# 2. Бейзлайн и оценка качества\n",
        "# 2.a Разделение на train/test\n",
        "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
        "    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        ")\n",
        "\n",
        "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -------------------- ВАЖНО! --------------------\n",
        "# Для базовой LogisticRegression добавим масштабирование.\n",
        "# Именно оно обычно решает проблему со сходимостью (или сильно облегчает задачу).\n",
        "# Для простоты, выполним его прямо здесь. Если вы хотите «чистый» бейзлайн,\n",
        "# то можно оставить данные как есть, но тогда увеличивать max_iter.\n",
        "# ------------------------------------------------\n",
        "scaler_class_baseline = StandardScaler()\n",
        "Xc_train_baseline_scaled = scaler_class_baseline.fit_transform(Xc_train)\n",
        "Xc_test_baseline_scaled = scaler_class_baseline.transform(Xc_test)\n",
        "\n",
        "# 2.b Классификация (Logistic Regression - Бейзлайн)\n",
        "logreg_baseline = LogisticRegression(max_iter=2000, random_state=42)\n",
        "logreg_baseline.fit(Xc_train_baseline_scaled, yc_train)\n",
        "yc_pred_baseline = logreg_baseline.predict(Xc_test_baseline_scaled)\n",
        "\n",
        "acc = accuracy_score(yc_test, yc_pred_baseline)\n",
        "prec = precision_score(yc_test, yc_pred_baseline)\n",
        "rec = recall_score(yc_test, yc_pred_baseline)\n",
        "f1 = f1_score(yc_test, yc_pred_baseline)\n",
        "\n",
        "print(\"\\nБейзлайн (Logistic Regression) - Classification:\")\n",
        "print(f\"Accuracy:  {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall:    {rec:.4f}\")\n",
        "print(f\"F1-score:  {f1:.4f}\")\n",
        "\n",
        "# 2.c Регрессия (Linear Regression - Бейзлайн)\n",
        "linreg_baseline = LinearRegression()\n",
        "linreg_baseline.fit(Xr_train, yr_train)\n",
        "yr_pred_baseline = linreg_baseline.predict(Xr_test)\n",
        "\n",
        "mse = mean_squared_error(yr_test, yr_pred_baseline)\n",
        "r2 = r2_score(yr_test, yr_pred_baseline)\n",
        "\n",
        "print(\"\\nБейзлайн (Linear Regression) - Regression:\")\n",
        "print(f\"MSE:  {mse:.4f}\")\n",
        "print(f\"R^2:  {r2:.4f}\")\n",
        "\n",
        "# 3. Улучшение бейзлайна\n",
        "# 3.a Обработка данных\n",
        "\n",
        "# Для классификации - масштабирование данных (уже было показано выше,\n",
        "# но для «улучшенной» модели можем ещё раз явно получить).\n",
        "scaler_class = StandardScaler()\n",
        "Xc_train_scaled = scaler_class.fit_transform(Xc_train)\n",
        "Xc_test_scaled = scaler_class.transform(Xc_test)\n",
        "\n",
        "# Для регрессии - добавление полиномиальных признаков + масштабирование\n",
        "scaler_reg = StandardScaler()\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "Xr_train_scaled = scaler_reg.fit_transform(Xr_train)\n",
        "Xr_test_scaled = scaler_reg.transform(Xr_test)\n",
        "\n",
        "# Преобразуем в полиномиальные признаки\n",
        "Xr_train_poly = poly.fit_transform(Xr_train_scaled)\n",
        "Xr_test_poly = poly.transform(Xr_test_scaled)\n",
        "\n",
        "# Допустим, попробуем для «улучшенной» регрессии Ridge\n",
        "ridge_reg = Ridge(alpha=1.0, random_state=42)\n",
        "ridge_reg.fit(Xr_train_poly, yr_train)\n",
        "yr_pred_ridge = ridge_reg.predict(Xr_test_poly)\n",
        "\n",
        "mse_ridge = mean_squared_error(yr_test, yr_pred_ridge)\n",
        "r2_ridge = r2_score(yr_test, yr_pred_ridge)\n",
        "\n",
        "print(\"\\nУлучшенная модель (Ridge + полиномиальные признаки) - Regression:\")\n",
        "print(f\"MSE:  {mse_ridge:.4f}\")\n",
        "print(f\"R^2:  {r2_ridge:.4f}\")\n",
        "\n",
        "# 3.b Улучшенная модель для классификации (Logistic Regression + GridSearchCV)\n",
        "pipeline_class = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(random_state=42, max_iter=2000))\n",
        "])\n",
        "\n",
        "param_grid_class = {\n",
        "    \"clf__C\": [0.01, 0.1, 1, 10, 100],\n",
        "    \"clf__solver\": [\"lbfgs\", \"liblinear\"]\n",
        "}\n",
        "\n",
        "grid_search_class = GridSearchCV(\n",
        "    estimator=pipeline_class,\n",
        "    param_grid=param_grid_class,\n",
        "    cv=5,\n",
        "    scoring=\"f1\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Обучаем на НЕмаcштабированных исходных Xc_train, так как внутри pipeline\n",
        "# есть свой StandardScaler. (Если уже есть масштабирование, можно было бы\n",
        "# подавать Xc_train_scaled, но тогда scaler в Pipeline избыточен.)\n",
        "grid_search_class.fit(Xc_train, yc_train)\n",
        "\n",
        "best_class_model = grid_search_class.best_estimator_\n",
        "yc_pred_improved = best_class_model.predict(Xc_test)\n",
        "\n",
        "acc_improved = accuracy_score(yc_test, yc_pred_improved)\n",
        "prec_improved = precision_score(yc_test, yc_pred_improved)\n",
        "rec_improved = recall_score(yc_test, yc_pred_improved)\n",
        "f1_improved = f1_score(yc_test, yc_pred_improved)\n",
        "\n",
        "print(\"\\nУлучшенная модель (Logistic Regression + GridSearchCV) - Classification:\")\n",
        "print(f\"Accuracy:  {acc_improved:.4f}\")\n",
        "print(f\"Precision: {prec_improved:.4f}\")\n",
        "print(f\"Recall:    {rec_improved:.4f}\")\n",
        "print(f\"F1-score:  {f1_improved:.4f}\")\n",
        "\n",
        "# 4. Собственные реализации моделей\n",
        "class CustomLinearRegression:\n",
        "    def __init__(self, lr=0.01, n_iter=1000):\n",
        "        self.lr = lr\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.array(X, dtype=float)\n",
        "        y = np.array(y, dtype=float)\n",
        "\n",
        "        # Добавляем столбец из 1 для свободного члена\n",
        "        ones = np.ones((X.shape[0], 1))\n",
        "        X = np.hstack([ones, X])\n",
        "\n",
        "        self.w_ = np.zeros(X.shape[1])\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            y_pred = X.dot(self.w_)\n",
        "            error = y_pred - y\n",
        "            grad = (1 / X.shape[0]) * X.T.dot(error)\n",
        "\n",
        "            # Проверка на NaN/Inf в градиенте\n",
        "            if np.any(np.isnan(grad)) or np.any(np.isinf(grad)):\n",
        "                print(f\"Iteration {i}: Gradient contains NaN or Inf values, stopping training.\")\n",
        "                break\n",
        "\n",
        "            self.w_ -= self.lr * grad\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X, dtype=float)\n",
        "        ones = np.ones((X.shape[0], 1))\n",
        "        X = np.hstack([ones, X])\n",
        "        return X.dot(self.w_)\n",
        "\n",
        "\n",
        "class CustomLogisticRegression:\n",
        "    def __init__(self, lr=0.0001, n_iter=1000):\n",
        "        self.lr = lr\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.array(X, dtype=float)\n",
        "        y = np.array(y, dtype=float)\n",
        "\n",
        "        # Добавляем столбец из 1 для свободного члена\n",
        "        ones = np.ones((X.shape[0], 1))\n",
        "        X = np.hstack([ones, X])\n",
        "\n",
        "        self.w_ = np.zeros(X.shape[1])\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            z = X.dot(self.w_)\n",
        "            y_pred = self._sigmoid(z)\n",
        "            error = y_pred - y\n",
        "            grad = (1 / X.shape[0]) * X.T.dot(error)\n",
        "\n",
        "            if np.any(np.isnan(grad)) or np.any(np.isinf(grad)):\n",
        "                print(f\"Iteration {i}: Gradient contains NaN or Inf values, stopping training.\")\n",
        "                break\n",
        "\n",
        "            self.w_ -= self.lr * grad\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X = np.array(X, dtype=float)\n",
        "        ones = np.ones((X.shape[0], 1))\n",
        "        X = np.hstack([ones, X])\n",
        "        return self._sigmoid(X.dot(self.w_))\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= threshold).astype(int)\n",
        "\n",
        "# Сравнение с библиотечными методами\n",
        "\n",
        "# Custom Linear Regression\n",
        "# Используем те же «улучшенные» данные (полиномиальные признаки + масштабирование)\n",
        "custom_linreg = CustomLinearRegression(lr=0.01, n_iter=1000)\n",
        "custom_linreg.fit(Xr_train_poly, yr_train)\n",
        "yr_pred_custom = custom_linreg.predict(Xr_test_poly)\n",
        "\n",
        "mse_custom = mean_squared_error(yr_test, yr_pred_custom)\n",
        "r2_custom = r2_score(yr_test, yr_pred_custom)\n",
        "\n",
        "print(\"\\nCustom Linear Regression - Regression:\")\n",
        "print(f\"MSE:  {mse_custom:.4f}\")\n",
        "print(f\"R^2:  {r2_custom:.4f}\")\n",
        "\n",
        "# Custom Logistic Regression\n",
        "# Возьмём масштабированные данные (без полиномиальных признаков, т.к. это классификация)\n",
        "custom_logreg = CustomLogisticRegression(lr=0.01, n_iter=2000)\n",
        "custom_logreg.fit(Xc_train_scaled, yc_train)\n",
        "yc_pred_custom = custom_logreg.predict(Xc_test_scaled)\n",
        "\n",
        "acc_custom = accuracy_score(yc_test, yc_pred_custom)\n",
        "prec_custom = precision_score(yc_test, yc_pred_custom)\n",
        "rec_custom = recall_score(yc_test, yc_pred_custom)\n",
        "f1_custom = f1_score(yc_test, yc_pred_custom)\n",
        "\n",
        "print(\"\\nCustom Logistic Regression - Classification:\")\n",
        "print(f\"Accuracy:  {acc_custom:.4f}\")\n",
        "print(f\"Precision: {prec_custom:.4f}\")\n",
        "print(f\"Recall:    {rec_custom:.4f}\")\n",
        "print(f\"F1-score:  {f1_custom:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjLZZYJhNh3A"
      },
      "source": [
        "Выводы:\n",
        "\n",
        "1. Базовые модели (Logistic Regression и Linear Regression из sklearn) продемонстрировали хорошие результаты на соответствующих задачах.\n",
        "\n",
        "2. Улучшение моделей посредством настройки гиперпараметров и создания полиномиальных признаков значительно повысило качество моделей.\n",
        "\n",
        "3. Собственные реализации моделей (CustomLinearRegression и CustomLogisticRegression) продемонстрировали конкурентоспособные, но все еще худшие, чем из sklearn результаты, что отмечает важность регуляризации и инженерии признаков для повышения качества модели.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Лабораторная работа 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHVJ7hC9_vGw",
        "outputId": "b9ed1d6e-ed1c-490a-9364-9170210e7726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Breast Cancer Wisconsin Dataset ===\n",
            "Feature matrix shape: (569, 30)\n",
            "Target vector shape:  (569,)\n",
            "Classes (0=malignant, 1=benign)\n",
            "\n",
            "=== Concrete Compressive Strength Dataset ===\n",
            "Feature matrix shape: (1030, 8)\n",
            "Target vector shape:  (1030, 1)\n",
            "\n",
            "=== Бейзлайн DecisionTree (Classification) ===\n",
            "Accuracy:  0.9123\n",
            "Precision: 0.9559\n",
            "Recall:    0.9028\n",
            "F1-score:  0.9286\n",
            "\n",
            "=== Бейзлайн DecisionTree (Regression) ===\n",
            "MSE:  42.0228\n",
            "R^2:  0.8369\n",
            "\n",
            "=== Улучшение бейзлайна: Классификация (Breast Cancer) ===\n",
            "Лучшие параметры: {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
            "Accuracy:  0.9211\n",
            "Precision: 0.9565\n",
            "Recall:    0.9167\n",
            "F1-score:  0.9362\n",
            "\n",
            "=== Улучшение бейзлайна: Регрессия (Concrete Strength) ===\n",
            "Лучшие параметры: {'dt_reg__criterion': 'squared_error', 'dt_reg__max_depth': None, 'dt_reg__min_samples_leaf': 1, 'dt_reg__min_samples_split': 2}\n",
            "MSE:  48.4651\n",
            "R^2:  0.8119\n",
            "\n",
            "=== Custom Decision Tree (Classification) ===\n",
            "Accuracy: 0.8596\n",
            "F1-score: 0.8857\n",
            "\n",
            "=== Custom Decision Tree (Regression) ===\n",
            "MSE: 197.8450\n",
            "R^2: 0.2322\n",
            "\n",
            "=== Custom Decision Tree Improved (Classification) ===\n",
            "Accuracy: 0.8772\n",
            "F1-score: 0.9079\n",
            "\n",
            "=== Custom Decision Tree Improved (Regression) ===\n",
            "MSE:  192.9455\n",
            "R^2:  0.2512\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# Лабораторная работа №3\n",
        "# (Применение решающего дерева для классификации и регрессии)\n",
        "# ==========================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# Модели, метрики, инструменты sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    mean_squared_error, mean_absolute_error, r2_score\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# -----------------------------------------\n",
        "# 1. Загрузка данных\n",
        "# -----------------------------------------\n",
        "\n",
        "# 1.a Данные для задачи классификации - Breast Cancer Wisconsin\n",
        "breast_cancer_data = load_breast_cancer()\n",
        "X_bc = breast_cancer_data.data\n",
        "y_bc = breast_cancer_data.target\n",
        "feature_names_bc = breast_cancer_data.feature_names\n",
        "\n",
        "print(\"=== Breast Cancer Wisconsin Dataset ===\")\n",
        "print(\"Feature matrix shape:\", X_bc.shape)\n",
        "print(\"Target vector shape: \", y_bc.shape)\n",
        "print(\"Classes (0=malignant, 1=benign)\")\n",
        "\n",
        "# 1.b Данные для задачи регрессии - Concrete Compressive Strength\n",
        "concrete_data = fetch_ucirepo(id=165)  # Concrete Compressive Strength Dataset\n",
        "X_conc = concrete_data.data.features\n",
        "y_conc = concrete_data.data.targets  # Это Series с целевым значением прочности\n",
        "feature_names_conc = X_conc.columns\n",
        "\n",
        "print(\"\\n=== Concrete Compressive Strength Dataset ===\")\n",
        "print(\"Feature matrix shape:\", X_conc.shape)\n",
        "print(\"Target vector shape: \", y_conc.shape)\n",
        "\n",
        "# -----------------------------------------\n",
        "# 2. Создание бейзлайна и оценка качества\n",
        "# -----------------------------------------\n",
        "\n",
        "# 2.a ОБУЧЕНИЕ (Classification)\n",
        "\n",
        "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
        "    X_bc, y_bc, test_size=0.2, random_state=42, stratify=y_bc\n",
        ")\n",
        "\n",
        "# Бейзлайновая модель: DecisionTreeClassifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(Xc_train, yc_train)\n",
        "yc_pred = dt_clf.predict(Xc_test)\n",
        "\n",
        "# 2.b ОЦЕНКА КАЧЕСТВА (Classification)\n",
        "acc = accuracy_score(yc_test, yc_pred)\n",
        "prec = precision_score(yc_test, yc_pred)\n",
        "rec = recall_score(yc_test, yc_pred)\n",
        "f1 = f1_score(yc_test, yc_pred)\n",
        "\n",
        "print(\"\\n=== Бейзлайн DecisionTree (Classification) ===\")\n",
        "print(f\"Accuracy:  {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall:    {rec:.4f}\")\n",
        "print(f\"F1-score:  {f1:.4f}\")\n",
        "\n",
        "# 2.a ОБУЧЕНИЕ (Regression)\n",
        "\n",
        "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
        "    X_conc, y_conc, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Бейзлайновая модель: DecisionTreeRegressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(Xr_train, yr_train)\n",
        "yr_pred = dt_reg.predict(Xr_test)\n",
        "\n",
        "# 2.b ОЦЕНКА КАЧЕСТВА (Regression)\n",
        "mse = mean_squared_error(yr_test, yr_pred)\n",
        "r2 = r2_score(yr_test, yr_pred)\n",
        "\n",
        "print(\"\\n=== Бейзлайн DecisionTree (Regression) ===\")\n",
        "print(f\"MSE:  {mse:.4f}\")\n",
        "print(f\"R^2:  {r2:.4f}\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# 3. Улучшение бейзлайна\n",
        "# -----------------------------------------\n",
        "\n",
        "# 3.a Формулировка гипотез улучшения:\n",
        "#    1) Подбор гиперпараметров решающего дерева (max_depth, min_samples_leaf, min_samples_split, ...)\n",
        "#    2) Масштабирование (для некоторых моделей), кодирование категориальных признаков (если были)\n",
        "#    3) Добавление новых признаков (если имеет смысл; в данном примере все признаки уже числовые)\n",
        "#    4) Удаление выбросов (при необходимости)\n",
        "\n",
        "# В нашем случае (Breast Cancer) все признаки числовые и уже относительно\n",
        "# отмасштабированы. Для примера - мы просто подберём гиперпараметры.\n",
        "\n",
        "print(\"\\n=== Улучшение бейзлайна: Классификация (Breast Cancer) ===\")\n",
        "\n",
        "param_grid_clf = {\n",
        "    'max_depth': [3, 5, None],\n",
        "    'min_samples_leaf': [1, 2, 5],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "grid_clf = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    param_grid_clf,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_clf.fit(Xc_train, yc_train)\n",
        "\n",
        "best_clf = grid_clf.best_estimator_\n",
        "yc_pred_best = best_clf.predict(Xc_test)\n",
        "\n",
        "acc_best = accuracy_score(yc_test, yc_pred_best)\n",
        "prec_best = precision_score(yc_test, yc_pred_best)\n",
        "rec_best = recall_score(yc_test, yc_pred_best)\n",
        "f1_best = f1_score(yc_test, yc_pred_best)\n",
        "\n",
        "print(\"Лучшие параметры:\", grid_clf.best_params_)\n",
        "print(f\"Accuracy:  {acc_best:.4f}\")\n",
        "print(f\"Precision: {prec_best:.4f}\")\n",
        "print(f\"Recall:    {rec_best:.4f}\")\n",
        "print(f\"F1-score:  {f1_best:.4f}\")\n",
        "\n",
        "print(\"\\n=== Улучшение бейзлайна: Регрессия (Concrete Strength) ===\")\n",
        "\n",
        "# Для демонстрации: попробуем добавить искусственный признак ratio_water_cement,\n",
        "# если он ещё не существует, — отношение воды к цементу,\n",
        "# полагая, что это может быть важным фактором в задаче.\n",
        "\n",
        "if 'Water' in X_conc.columns and 'Cement' in X_conc.columns:\n",
        "    X_conc['ratio_water_cement'] = X_conc['Water'] / (X_conc['Cement'] + 1e-5)\n",
        "\n",
        "# Разбиваем заново, теперь уже с новым признаком\n",
        "Xr_train2, Xr_test2, yr_train2, yr_test2 = train_test_split(\n",
        "    X_conc, y_conc, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Создадим пайплайн для подбора гиперпараметров\n",
        "# (при желании можно добавить StandardScaler, но DecisionTree не слишком\n",
        "#  чувствительна к масштабу)\n",
        "pipe_reg = Pipeline([\n",
        "    (\"dt_reg\", DecisionTreeRegressor(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid_reg = {\n",
        "    \"dt_reg__max_depth\": [3, 5, 7, None],\n",
        "    \"dt_reg__min_samples_leaf\": [1, 2, 5],\n",
        "    \"dt_reg__min_samples_split\": [2, 5, 10],\n",
        "    \"dt_reg__criterion\": [\"squared_error\", \"absolute_error\"]\n",
        "}\n",
        "\n",
        "grid_reg = GridSearchCV(\n",
        "    pipe_reg,\n",
        "    param_grid_reg,\n",
        "    cv=5,\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_reg.fit(Xr_train2, yr_train2)\n",
        "best_reg = grid_reg.best_estimator_\n",
        "yr_pred_best = best_reg.predict(Xr_test2)\n",
        "\n",
        "mse_best = mean_squared_error(yr_test2, yr_pred_best)\n",
        "r2_best = r2_score(yr_test2, yr_pred_best)\n",
        "\n",
        "print(\"Лучшие параметры:\", grid_reg.best_params_)\n",
        "print(f\"MSE:  {mse_best:.4f}\")\n",
        "print(f\"R^2:  {r2_best:.4f}\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# 4. Имплементация \"Custom Decision Tree\"\n",
        "#    (здесь - упрощённый пример)\n",
        "# -----------------------------------------\n",
        "from collections import Counter\n",
        "\n",
        "class CustomDecisionTreeClassifier:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X, y = np.array(X), np.array(y)\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "        return self\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        # 1) критерии остановки\n",
        "        if len(set(y)) == 1:\n",
        "            return {'type':'leaf','class':y[0]}\n",
        "        if (self.max_depth is not None) and (depth >= self.max_depth):\n",
        "            # Мажоритарный класс\n",
        "            return {'type':'leaf','class': Counter(y).most_common(1)[0][0]}\n",
        "        if len(X) < self.min_samples_split:\n",
        "            return {'type':'leaf','class': Counter(y).most_common(1)[0][0]}\n",
        "\n",
        "        # 2) Упрощённый выбор лучшего признака: первый признак и медиана\n",
        "        best_feat = 0\n",
        "        best_thresh = np.median(X[:,0])\n",
        "\n",
        "        left_idx = X[:,best_feat] <= best_thresh\n",
        "        right_idx = ~left_idx\n",
        "\n",
        "        node = {\n",
        "            'type': 'node',\n",
        "            'feature': best_feat,\n",
        "            'thresh': best_thresh,\n",
        "            'left':  self._build_tree(X[left_idx],  y[left_idx],  depth+1),\n",
        "            'right': self._build_tree(X[right_idx], y[right_idx], depth+1)\n",
        "        }\n",
        "        return node\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        preds = []\n",
        "        for x in X:\n",
        "            preds.append(self._traverse(self.root, x))\n",
        "        return np.array(preds)\n",
        "\n",
        "    def _traverse(self, node, x):\n",
        "        if node['type'] == 'leaf':\n",
        "            return node['class']\n",
        "        else:\n",
        "            if x[node['feature']] <= node['thresh']:\n",
        "                return self._traverse(node['left'], x)\n",
        "            else:\n",
        "                return self._traverse(node['right'], x)\n",
        "\n",
        "class CustomDecisionTreeRegressor:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X, y = np.array(X), np.array(y)\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "        return self\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        # Критерии остановки\n",
        "        if (self.max_depth is not None) and (depth >= self.max_depth):\n",
        "            return {'type':'leaf', 'value': y.mean()}\n",
        "        if len(X) < self.min_samples_split:\n",
        "            return {'type':'leaf', 'value': y.mean()}\n",
        "        if np.allclose(y, y[0]):\n",
        "            return {'type':'leaf', 'value': y[0]}\n",
        "\n",
        "        # Упрощённый вариант сплита - первый признак, медиана\n",
        "        best_feat = 0\n",
        "        best_thresh = np.median(X[:,0])\n",
        "\n",
        "        left_idx = X[:,best_feat] <= best_thresh\n",
        "        right_idx = ~left_idx\n",
        "\n",
        "        node = {\n",
        "            'type': 'node',\n",
        "            'feature': best_feat,\n",
        "            'thresh': best_thresh,\n",
        "            'left':  self._build_tree(X[left_idx], y[left_idx], depth+1),\n",
        "            'right': self._build_tree(X[right_idx], y[right_idx], depth+1)\n",
        "        }\n",
        "        return node\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        preds = []\n",
        "        for x in X:\n",
        "            preds.append(self._traverse(self.root, x))\n",
        "        return np.array(preds)\n",
        "\n",
        "    def _traverse(self, node, x):\n",
        "        if node['type'] == 'leaf':\n",
        "            return node['value']\n",
        "        else:\n",
        "            if x[node['feature']] <= node['thresh']:\n",
        "                return self._traverse(node['left'], x)\n",
        "            else:\n",
        "                return self._traverse(node['right'], x)\n",
        "\n",
        "# -----------------------------------------\n",
        "# 4.a-b Обучение кастомных моделей и оценка\n",
        "# -----------------------------------------\n",
        "\n",
        "# Классификация на Breast Cancer\n",
        "cust_clf = CustomDecisionTreeClassifier(max_depth=3, min_samples_split=10)\n",
        "cust_clf.fit(Xc_train, yc_train)\n",
        "yc_pred_custom = cust_clf.predict(Xc_test)\n",
        "\n",
        "acc_cust = accuracy_score(yc_test, yc_pred_custom)\n",
        "f1_cust = f1_score(yc_test, yc_pred_custom)\n",
        "\n",
        "print(\"\\n=== Custom Decision Tree (Classification) ===\")\n",
        "print(f\"Accuracy: {acc_cust:.4f}\")\n",
        "print(f\"F1-score: {f1_cust:.4f}\")\n",
        "\n",
        "# Регрессия на Concrete Strength\n",
        "cust_reg = CustomDecisionTreeRegressor(max_depth=3, min_samples_split=10)\n",
        "cust_reg.fit(Xr_train, yr_train)\n",
        "yr_pred_custom = cust_reg.predict(Xr_test)\n",
        "\n",
        "mse_cust = mean_squared_error(yr_test, yr_pred_custom)\n",
        "r2_cust = r2_score(yr_test, yr_pred_custom)\n",
        "\n",
        "print(\"\\n=== Custom Decision Tree (Regression) ===\")\n",
        "print(f\"MSE: {mse_cust:.4f}\")\n",
        "print(f\"R^2: {r2_cust:.4f}\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# 4.f-g Применение улучшений для кастомной модели\n",
        "# -----------------------------------------\n",
        "\n",
        "# Пример небольшого улучшения: добавляем искусственный признак\n",
        "# (Breast Cancer: возьмём два первых признака и сделаем их сумму)\n",
        "\n",
        "Xc_train_enh = np.column_stack([Xc_train, Xc_train[:,0] + Xc_train[:,1]])\n",
        "Xc_test_enh = np.column_stack([Xc_test,  Xc_test[:,0]  + Xc_test[:,1]])\n",
        "\n",
        "cust_clf_improved = CustomDecisionTreeClassifier(max_depth=5, min_samples_split=5)\n",
        "cust_clf_improved.fit(Xc_train_enh, yc_train)\n",
        "yc_pred_cust_improved = cust_clf_improved.predict(Xc_test_enh)\n",
        "\n",
        "acc_cust_improved = accuracy_score(yc_test, yc_pred_cust_improved)\n",
        "f1_cust_improved = f1_score(yc_test, yc_pred_cust_improved)\n",
        "\n",
        "print(\"\\n=== Custom Decision Tree Improved (Classification) ===\")\n",
        "print(f\"Accuracy: {acc_cust_improved:.4f}\")\n",
        "print(f\"F1-score: {f1_cust_improved:.4f}\")\n",
        "\n",
        "# Аналогично для регрессии (Concrete Strength):\n",
        "# Добавим признак = сумма Cement и Water\n",
        "Xr_train_enh = Xr_train.copy()\n",
        "Xr_test_enh = Xr_test.copy()\n",
        "\n",
        "if 'Cement' in Xr_train.columns and 'Water' in Xr_train.columns:\n",
        "    Xr_train_enh['cement_water_sum'] = Xr_train['Cement'] + Xr_train['Water']\n",
        "    Xr_test_enh['cement_water_sum']  = Xr_test['Cement'] + Xr_test['Water']\n",
        "\n",
        "cust_reg_improved = CustomDecisionTreeRegressor(max_depth=5, min_samples_split=5)\n",
        "cust_reg_improved.fit(Xr_train_enh, yr_train)\n",
        "yr_pred_cust_improved = cust_reg_improved.predict(Xr_test_enh)\n",
        "\n",
        "mse_cust_improved = mean_squared_error(yr_test, yr_pred_cust_improved)\n",
        "r2_cust_improved = r2_score(yr_test, yr_pred_cust_improved)\n",
        "\n",
        "print(\"\\n=== Custom Decision Tree Improved (Regression) ===\")\n",
        "print(f\"MSE:  {mse_cust_improved:.4f}\")\n",
        "print(f\"R^2:  {r2_cust_improved:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPSyjoobNIm-"
      },
      "source": [
        "Выводы:\n",
        "1. Бейзлайновые модели sklearn (DecisionTreeClassifier/Regressor) обычно дают\n",
        "   более качественные предсказания, чем простая кастомная реализация,\n",
        "   потому что в sklearn применены более совершенные алгоритмы поиска оптимального сплита.\n",
        "2. Подбор гиперпараметров и добавление новых признаков улучшают показатели\n",
        "   как в sklearn-модели, так и в кастомной.\n",
        "3. В реальных задачах рекомендуется использовать хорошо отлаженные библиотеки, а кастомные реализации полезны\n",
        "   только в образовательных/исследовательских целях, чтобы глубже понять логику работы алгоритма."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Лабораторная работа 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1cCwIfHA7oK",
        "outputId": "218e036d-4585-45c7-cea3-e6ae622649fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Breast Cancer data shape: (569, 30)\n",
            "Concrete data shape: (1030, 8)\n",
            "\n",
            "Бейзлайн RandomForest (Classification) на Breast Cancer:\n",
            "Accuracy:  0.9474\n",
            "Precision: 0.9583\n",
            "Recall:    0.9583\n",
            "F1-score:  0.9583\n",
            "\n",
            "Бейзлайн RandomForest (Regression) на Concrete Compressive Strength:\n",
            "MSE:  20.9711\n",
            "R^2:  0.9204\n",
            "\n",
            "Улучшенный RandomForest (Classification):\n",
            "Лучшие параметры: {'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 50}\n",
            "Accuracy:  0.9474\n",
            "Precision: 0.9583\n",
            "Recall:    0.9583\n",
            "F1-score:  0.9583\n",
            "\n",
            "Улучшенный RandomForest (Regression):\n",
            "Лучшие параметры: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100}\n",
            "MSE:  21.0961\n",
            "R^2:  0.9199\n",
            "Обучение и оценка собственной реализации RandomForest (Classification)\n",
            "\n",
            "[Custom RandomForestClassifier] (Breast Cancer)\n",
            "Accuracy:  0.9474\n",
            "Precision: 0.9459\n",
            "Recall:    0.9722\n",
            "F1-score:  0.9589\n",
            "Обучение и оценка собственной реализации RandomForest (Regression)\n",
            "\n",
            "[Custom RandomForestRegressor] (Concrete)\n",
            "MSE:  98.2264\n",
            "R^2:  0.6271\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             mean_squared_error, mean_absolute_error, r2_score)\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from collections import Counter\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "###############################################################################\n",
        "# 1) Загрузка данных\n",
        "###############################################################################\n",
        "\n",
        "# --- (A) Датасет для классификации: Breast Cancer Wisconsin ---\n",
        "breast_data = load_breast_cancer()\n",
        "X_bc = pd.DataFrame(breast_data.data, columns=breast_data.feature_names)\n",
        "y_bc = pd.Series(breast_data.target, name='target')  # превращаем в Series\n",
        "\n",
        "print(\"Breast Cancer data shape:\", X_bc.shape)\n",
        "\n",
        "# --- (B) Датасет для регрессии: Concrete Compressive Strength ---\n",
        "concrete_dataset = fetch_ucirepo(id=165)  # ID=165 - Concrete Compressive Strength\n",
        "X_cc = concrete_dataset.data.features\n",
        "y_cc = concrete_dataset.data.targets\n",
        "\n",
        "print(\"Concrete data shape:\", X_cc.shape)\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 2) Первичная обработка / Бейзлайн\n",
        "###############################################################################\n",
        "\n",
        "# --- 2.A Классификация (Breast Cancer) ---\n",
        "\n",
        "# Пример: удаляем строки, где mean texture > 40 (условные \"выбросы\")\n",
        "X_bc_clean = X_bc[X_bc['mean texture'] < 40].copy()\n",
        "y_bc_clean = y_bc.loc[X_bc_clean.index].squeeze()\n",
        "\n",
        "# Создадим новый признак, например: mean radius * mean smoothness\n",
        "X_bc_clean['radius_smoothness'] = X_bc_clean['mean radius'] * X_bc_clean['mean smoothness']\n",
        "\n",
        "# Разделение на train/test\n",
        "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
        "    X_bc_clean, y_bc_clean, test_size=0.2, random_state=0, stratify=y_bc_clean\n",
        ")\n",
        "\n",
        "# Бейзлайн: RandomForestClassifier без подбора гиперпараметров\n",
        "rf_clf = RandomForestClassifier(random_state=0)\n",
        "rf_clf.fit(Xc_train, y_bc_clean.loc[Xc_train.index].to_numpy())  # Используем to_numpy()\n",
        "yc_pred = rf_clf.predict(Xc_test)\n",
        "\n",
        "acc = accuracy_score(yc_test, yc_pred)\n",
        "prec = precision_score(yc_test, yc_pred)\n",
        "rec = recall_score(yc_test, yc_pred)\n",
        "f1 = f1_score(yc_test, yc_pred)\n",
        "\n",
        "print(\"\\nБейзлайн RandomForest (Classification) на Breast Cancer:\")\n",
        "print(f\"Accuracy:  {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall:    {rec:.4f}\")\n",
        "print(f\"F1-score:  {f1:.4f}\")\n",
        "\n",
        "\n",
        "# --- 2.B Регрессия (Concrete) ---\n",
        "\n",
        "# Пример: удаляем строки, где Cement > 550 (условные \"выбросы\")\n",
        "X_cc_clean = X_cc[X_cc['Cement'] <= 550].copy()\n",
        "y_cc_clean = y_cc.loc[X_cc_clean.index].squeeze()\n",
        "\n",
        "# Создадим новый признак: Water * Age\n",
        "X_cc_clean['Water*Age'] = X_cc_clean['Water'] * X_cc_clean['Age']\n",
        "\n",
        "# Разделение на train/test\n",
        "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
        "    X_cc_clean, y_cc_clean, test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "# Бейзлайн: RandomForestRegressor без подбора гиперпараметров\n",
        "rf_reg = RandomForestRegressor(random_state=0)\n",
        "rf_reg.fit(Xr_train, y_cc_clean.loc[Xr_train.index].to_numpy())  # Используем to_numpy()\n",
        "yr_pred = rf_reg.predict(Xr_test)\n",
        "\n",
        "mse = mean_squared_error(yr_test, yr_pred)\n",
        "r2 = r2_score(yr_test, yr_pred)\n",
        "\n",
        "print(\"\\nБейзлайн RandomForest (Regression) на Concrete Compressive Strength:\")\n",
        "print(f\"MSE:  {mse:.4f}\")\n",
        "print(f\"R^2:  {r2:.4f}\")\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 3) Улучшение бейзлайна (подбор гиперпараметров)\n",
        "###############################################################################\n",
        "\n",
        "# --- 3.A Classification (Breast Cancer) ---\n",
        "param_grid_clf = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [3, 5, None],\n",
        "    'min_samples_leaf': [1, 2, 5],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "rf_clf_cv = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=0),\n",
        "    param_grid_clf,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_clf_cv.fit(Xc_train, y_bc_clean.loc[Xc_train.index].to_numpy())  # Используем to_numpy()\n",
        "best_clf = rf_clf_cv.best_estimator_\n",
        "yc_pred_best = best_clf.predict(Xc_test)\n",
        "\n",
        "acc_best = accuracy_score(yc_test, yc_pred_best)\n",
        "prec_best = precision_score(yc_test, yc_pred_best)\n",
        "rec_best = recall_score(yc_test, yc_pred_best)\n",
        "f1_best = f1_score(yc_test, yc_pred_best)\n",
        "\n",
        "print(\"\\nУлучшенный RandomForest (Classification):\")\n",
        "print(\"Лучшие параметры:\", rf_clf_cv.best_params_)\n",
        "print(f\"Accuracy:  {acc_best:.4f}\")\n",
        "print(f\"Precision: {prec_best:.4f}\")\n",
        "print(f\"Recall:    {rec_best:.4f}\")\n",
        "print(f\"F1-score:  {f1_best:.4f}\")\n",
        "\n",
        "\n",
        "# --- 3.B Regression (Concrete) ---\n",
        "param_grid_reg = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [3, 5, None],\n",
        "    'min_samples_leaf': [1, 2, 5],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "rf_reg_cv = GridSearchCV(\n",
        "    RandomForestRegressor(random_state=0),\n",
        "    param_grid_reg,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_reg_cv.fit(Xr_train, y_cc_clean.loc[Xr_train.index].to_numpy())  # Используем to_numpy()\n",
        "best_reg = rf_reg_cv.best_estimator_\n",
        "yr_pred_best = best_reg.predict(Xr_test)\n",
        "\n",
        "mse_best = mean_squared_error(yr_test, yr_pred_best)\n",
        "r2_best = r2_score(yr_test, yr_pred_best)\n",
        "\n",
        "print(\"\\nУлучшенный RandomForest (Regression):\")\n",
        "print(\"Лучшие параметры:\", rf_reg_cv.best_params_)\n",
        "print(f\"MSE:  {mse_best:.4f}\")\n",
        "print(f\"R^2:  {r2_best:.4f}\")\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# 4) Имплементация собственного (упрощённого) RandomForest\n",
        "###############################################################################\n",
        "\n",
        "class SimpleDecisionTreeClassifier:\n",
        "    \"\"\"\n",
        "    Очень упрощённое дерево для классификации.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, max_features=None, random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "        self.root = None\n",
        "        if self.random_state is not None:\n",
        "            self.random_state_ = np.random.RandomState(self.random_state)\n",
        "        else:\n",
        "            self.random_state_ = np.random.RandomState()\n",
        "\n",
        "    def _get_features_subset(self, n_features):\n",
        "        \"\"\"Выбираем случайное подмножество признаков согласно max_features\"\"\"\n",
        "        if self.max_features is None:\n",
        "            return np.arange(n_features)\n",
        "        elif self.max_features == 'sqrt':\n",
        "            k = max(1, int(np.sqrt(n_features)))\n",
        "            return self.random_state_.choice(n_features, k, replace=False)\n",
        "        elif isinstance(self.max_features, int):\n",
        "            k = min(self.max_features, n_features)\n",
        "            return self.random_state_.choice(n_features, k, replace=False)\n",
        "        else:\n",
        "            raise ValueError(\"max_features должно быть 'sqrt', int или None\")\n",
        "\n",
        "    def fit(self, X, y, depth=0):\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        # Проверка на пустые\n",
        "        if len(X) == 0 or len(y) == 0:\n",
        "            self.root = ('leaf', 0)\n",
        "            return self\n",
        "\n",
        "        # Критерии остановки\n",
        "        if len(np.unique(y)) == 1 or len(X) < self.min_samples_split:\n",
        "            self.root = ('leaf', Counter(y).most_common(1)[0][0])\n",
        "            return self\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            self.root = ('leaf', Counter(y).most_common(1)[0][0])\n",
        "            return self\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "        features_subset = self._get_features_subset(n_features)\n",
        "\n",
        "        best_feat = None\n",
        "        best_thresh = None\n",
        "        best_balance = len(X)\n",
        "\n",
        "        for feat in features_subset:\n",
        "            thresh_candidate = np.median(X[:, feat])\n",
        "            left_idx = (X[:, feat] <= thresh_candidate)\n",
        "            right_idx = ~left_idx\n",
        "\n",
        "            # если одна из сторон пуста - пропускаем\n",
        "            if left_idx.sum() == 0 or right_idx.sum() == 0:\n",
        "                continue\n",
        "\n",
        "            # простой критерий \"баланс\"\n",
        "            balance = abs(left_idx.sum() - right_idx.sum())\n",
        "            if balance < best_balance:\n",
        "                best_balance = balance\n",
        "                best_feat = feat\n",
        "                best_thresh = thresh_candidate\n",
        "\n",
        "        if best_feat is None:\n",
        "            # если не нашли признака, делаем лист\n",
        "            self.root = ('leaf', Counter(y).most_common(1)[0][0])\n",
        "            return self\n",
        "\n",
        "        left_idx = (X[:, best_feat] <= best_thresh)\n",
        "        right_idx = ~left_idx\n",
        "\n",
        "        # Генерируем новые случайные состояния для поддеревьев\n",
        "        left_random_state = self.random_state_.randint(0, 10000)\n",
        "        right_random_state = self.random_state_.randint(0, 10000)\n",
        "\n",
        "        left_tree = SimpleDecisionTreeClassifier(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_split=self.min_samples_split,\n",
        "            max_features=self.max_features,\n",
        "            random_state=left_random_state\n",
        "        ).fit(X[left_idx], y[left_idx], depth+1)\n",
        "\n",
        "        right_tree = SimpleDecisionTreeClassifier(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_split=self.min_samples_split,\n",
        "            max_features=self.max_features,\n",
        "            random_state=right_random_state\n",
        "        ).fit(X[right_idx], y[right_idx], depth+1)\n",
        "\n",
        "        self.root = ('node', best_feat, best_thresh, left_tree, right_tree)\n",
        "        return self\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        node = self.root\n",
        "        while True:\n",
        "            if node[0] == 'leaf':\n",
        "                return node[1]\n",
        "            else:\n",
        "                _, feat, thresh, left_subtree, right_subtree = node\n",
        "                if x[feat] <= thresh:\n",
        "                    node = left_subtree.root\n",
        "                else:\n",
        "                    node = right_subtree.root\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        return np.array([self.predict_one(x) for x in X])\n",
        "\n",
        "\n",
        "class SimpleDecisionTreeRegressor:\n",
        "    \"\"\"\n",
        "    Упрощённое дерево для регрессии.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, max_features=None, random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "        self.root = None\n",
        "        if self.random_state is not None:\n",
        "            self.random_state_ = np.random.RandomState(self.random_state)\n",
        "        else:\n",
        "            self.random_state_ = np.random.RandomState()\n",
        "\n",
        "    def _get_features_subset(self, n_features):\n",
        "        \"\"\"Выбираем случайное подмножество признаков согласно max_features\"\"\"\n",
        "        if self.max_features is None:\n",
        "            return np.arange(n_features)\n",
        "        elif self.max_features == 'sqrt':\n",
        "            k = max(1, int(np.sqrt(n_features)))\n",
        "            return self.random_state_.choice(n_features, k, replace=False)\n",
        "        elif isinstance(self.max_features, int):\n",
        "            k = min(self.max_features, n_features)\n",
        "            return self.random_state_.choice(n_features, k, replace=False)\n",
        "        else:\n",
        "            raise ValueError(\"max_features должно быть 'sqrt', int или None\")\n",
        "\n",
        "    def fit(self, X, y, depth=0):\n",
        "        X = np.array(X)\n",
        "        y = np.array(y, dtype=float)\n",
        "\n",
        "        # Проверка на пустые\n",
        "        if len(X) == 0 or len(y) == 0:\n",
        "            self.root = ('leaf', 0.0)\n",
        "            return self\n",
        "\n",
        "        # Критерии остановки\n",
        "        if len(X) < self.min_samples_split:\n",
        "            self.root = ('leaf', np.mean(y))\n",
        "            return self\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            self.root = ('leaf', np.mean(y))\n",
        "            return self\n",
        "        if np.allclose(y, y[0]):\n",
        "            self.root = ('leaf', y[0])\n",
        "            return self\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "        features_subset = self._get_features_subset(n_features)\n",
        "\n",
        "        best_feat = None\n",
        "        best_thresh = None\n",
        "        best_balance = len(X)\n",
        "\n",
        "        for feat in features_subset:\n",
        "            thresh_candidate = np.median(X[:, feat])\n",
        "            left_idx = (X[:, feat] <= thresh_candidate)\n",
        "            right_idx = ~left_idx\n",
        "\n",
        "            # если одна из сторон пуста - пропускаем\n",
        "            if left_idx.sum() == 0 or right_idx.sum() == 0:\n",
        "                continue\n",
        "\n",
        "            # простой критерий \"баланс\"\n",
        "            balance = abs(left_idx.sum() - right_idx.sum())\n",
        "            if balance < best_balance:\n",
        "                best_balance = balance\n",
        "                best_feat = feat\n",
        "                best_thresh = thresh_candidate\n",
        "\n",
        "        if best_feat is None:\n",
        "            # если не нашли признака, делаем лист\n",
        "            self.root = ('leaf', np.mean(y))\n",
        "            return self\n",
        "\n",
        "        left_idx = (X[:, best_feat] <= best_thresh)\n",
        "        right_idx = ~left_idx\n",
        "\n",
        "        # Генерируем новые случайные состояния для поддеревьев\n",
        "        left_random_state = self.random_state_.randint(0, 10000)\n",
        "        right_random_state = self.random_state_.randint(0, 10000)\n",
        "\n",
        "        left_tree = SimpleDecisionTreeRegressor(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_split=self.min_samples_split,\n",
        "            max_features=self.max_features,\n",
        "            random_state=left_random_state\n",
        "        ).fit(X[left_idx], y[left_idx], depth+1)\n",
        "\n",
        "        right_tree = SimpleDecisionTreeRegressor(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_split=self.min_samples_split,\n",
        "            max_features=self.max_features,\n",
        "            random_state=right_random_state\n",
        "        ).fit(X[right_idx], y[right_idx], depth+1)\n",
        "\n",
        "        self.root = ('node', best_feat, best_thresh, left_tree, right_tree)\n",
        "        return self\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        node = self.root\n",
        "        while True:\n",
        "            if node[0] == 'leaf':\n",
        "                return node[1]\n",
        "            else:\n",
        "                _, feat, thresh, left_subtree, right_subtree = node\n",
        "                if x[feat] <= thresh:\n",
        "                    node = left_subtree.root\n",
        "                else:\n",
        "                    node = right_subtree.root\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        return np.array([self.predict_one(x) for x in X])\n",
        "\n",
        "\n",
        "class SimpleRandomForestClassifier:\n",
        "    \"\"\"\n",
        "    Упрощённая реализация RandomForest для классификации.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=50, max_depth=None, min_samples_split=2,\n",
        "                 max_features='sqrt', random_state=0):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        self.trees = []\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            # Bootstrap\n",
        "            indices = rng.choice(n_samples, n_samples, replace=True)\n",
        "            X_boot = X[indices]\n",
        "            y_boot = y[indices]\n",
        "\n",
        "            # Создаём дерево с уникальным random_state для разнообразия\n",
        "            tree_random_state = rng.randint(0, 10000)\n",
        "            tree = SimpleDecisionTreeClassifier(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                max_features=self.max_features,\n",
        "                random_state=tree_random_state\n",
        "            )\n",
        "            tree.fit(X_boot, y_boot)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        # Сбор предсказаний от всех деревьев\n",
        "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
        "        # Транспонируем для удобства\n",
        "        predictions = predictions.T  # shape: (n_samples, n_estimators)\n",
        "\n",
        "        final_preds = []\n",
        "        for i in range(predictions.shape[0]):\n",
        "            column = predictions[i]\n",
        "            most_common = Counter(column).most_common(1)[0][0]\n",
        "            final_preds.append(most_common)\n",
        "        return np.array(final_preds)\n",
        "\n",
        "\n",
        "class SimpleRandomForestRegressor:\n",
        "    \"\"\"\n",
        "    Упрощённая реализация RandomForest для регрессии.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=50, max_depth=None, min_samples_split=2,\n",
        "                 max_features='sqrt', random_state=0):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        self.trees = []\n",
        "        rng = np.random.RandomState(self.random_state)\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            # Bootstrap\n",
        "            indices = rng.choice(n_samples, n_samples, replace=True)\n",
        "            X_boot = X[indices]\n",
        "            y_boot = y[indices]\n",
        "\n",
        "            # Создаём дерево с уникальным random_state для разнообразия\n",
        "            tree_random_state = rng.randint(0, 10000)\n",
        "            tree = SimpleDecisionTreeRegressor(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                max_features=self.max_features,\n",
        "                random_state=tree_random_state\n",
        "            )\n",
        "            tree.fit(X_boot, y_boot)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        # Сбор предсказаний от всех деревьев\n",
        "        all_preds = np.array([tree.predict(X) for tree in self.trees])  # shape: (n_estimators, n_samples)\n",
        "        # Среднее по деревьям\n",
        "        return np.mean(all_preds, axis=0)\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Обучение и Оценка Собственной Реализации RandomForest (Классификация)\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "print(\"Обучение и оценка собственной реализации RandomForest (Classification)\")\n",
        "\n",
        "\n",
        "# Инициализация собственной реализации RandomForestClassifier\n",
        "cust_rf_clf = SimpleRandomForestClassifier(\n",
        "    n_estimators=50,\n",
        "    max_depth=5,\n",
        "    min_samples_split=2,\n",
        "    max_features='sqrt',\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "# Обучение на тех же данных\n",
        "cust_rf_clf.fit(Xc_train, yc_train)\n",
        "\n",
        "# Предсказание на тестовой выборке\n",
        "yc_pred_custom = cust_rf_clf.predict(Xc_test)\n",
        "\n",
        "# Вычисление метрик\n",
        "acc_cust = accuracy_score(yc_test, yc_pred_custom)\n",
        "prec_cust = precision_score(yc_test, yc_pred_custom)\n",
        "rec_cust = recall_score(yc_test, yc_pred_custom)\n",
        "f1_cust = f1_score(yc_test, yc_pred_custom)\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"\\n[Custom RandomForestClassifier] (Breast Cancer)\")\n",
        "print(f\"Accuracy:  {acc_cust:.4f}\")\n",
        "print(f\"Precision: {prec_cust:.4f}\")\n",
        "print(f\"Recall:    {rec_cust:.4f}\")\n",
        "print(f\"F1-score:  {f1_cust:.4f}\")\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Обучение и Оценка Собственной Реализации RandomForest (Регрессия)\n",
        "###############################################################################\n",
        "\n",
        "print(\"Обучение и оценка собственной реализации RandomForest (Regression)\")\n",
        "\n",
        "\n",
        "# Инициализация собственной реализации RandomForestRegressor\n",
        "cust_rf_reg = SimpleRandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    max_depth=5,\n",
        "    min_samples_split=2,\n",
        "    max_features='sqrt',\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "# Обучение на тех же данных\n",
        "cust_rf_reg.fit(Xr_train, yr_train)\n",
        "\n",
        "# Предсказание на тестовой выборке\n",
        "yr_pred_custom = cust_rf_reg.predict(Xr_test)\n",
        "\n",
        "# Вычисление метрик\n",
        "mse_cust = mean_squared_error(yr_test, yr_pred_custom)\n",
        "r2_cust = r2_score(yr_test, yr_pred_custom)\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"\\n[Custom RandomForestRegressor] (Concrete)\")\n",
        "print(f\"MSE:  {mse_cust:.4f}\")\n",
        "print(f\"R^2:  {r2_cust:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET733Fy0RQPH"
      },
      "source": [
        "Выводы:\n",
        "\n",
        "1. Базовые модели (RandomForestClassifier и RandomForestRegressor из sklearn) показали высокую эффективность.\n",
        "\n",
        "2. Оптимизация гиперпараметров с помощью GridSearchCV не привела к существенному улучшению:\n",
        "Классификация: Улучшенная модель показала аналогичные результаты базовой модели, что свидетельствует о близости гиперпараметров к оптимальным.\n",
        "Регрессия: Небольшое ухудшение показателей после настройки, что может быть связано с переобучением или неидеальным подбором параметров.\n",
        "\n",
        "3. Собственные реализации моделей продемонстрировали разную эффективность:\n",
        "Классификация: Custom RandomForestClassifier показал результаты, сопоставимые с моделями sklearn, подтверждая корректную реализацию алгоритма.\n",
        "Регрессия: Custom RandomForestRegressor значительно уступил библиотечным моделям по MSE и R², указывая на необходимость доработки алгоритма.\n",
        "\n",
        "4. Готовые реализации RandomForest из sklearn обеспечивают высокую производительность и надежность для практических задач. Разработка собственных моделей полезна для глубокого понимания алгоритмов, однако требует дальнейшей оптимизации для достижения конкурентоспособных результатов, особенно в задачах регрессии."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Лабораторная работа 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C3BbZ0oOZRV",
        "outputId": "7e970393-5791-4fa3-8e6e-1adde052e640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Breast Cancer Dataset shape: (569, 30)\n",
            "Классы меток: [0 1]\n",
            "\n",
            "Структура concrete_compressive_strength.variables:\n",
            "                            name     role        type demographic description  \\\n",
            "0                         Cement  Feature  Continuous        None        None   \n",
            "1             Blast Furnace Slag  Feature     Integer        None        None   \n",
            "2                        Fly Ash  Feature  Continuous        None        None   \n",
            "3                          Water  Feature  Continuous        None        None   \n",
            "4               Superplasticizer  Feature  Continuous        None        None   \n",
            "5               Coarse Aggregate  Feature  Continuous        None        None   \n",
            "6                 Fine Aggregate  Feature  Continuous        None        None   \n",
            "7                            Age  Feature     Integer        None        None   \n",
            "8  Concrete compressive strength   Target  Continuous        None        None   \n",
            "\n",
            "    units missing_values  \n",
            "0  kg/m^3             no  \n",
            "1  kg/m^3             no  \n",
            "2  kg/m^3             no  \n",
            "3  kg/m^3             no  \n",
            "4  kg/m^3             no  \n",
            "5  kg/m^3             no  \n",
            "6  kg/m^3             no  \n",
            "7     day             no  \n",
            "8     MPa             no  \n",
            "\n",
            "Concrete Compressive Strength Dataset shape: (1030, 8)\n",
            "Первые 5 строк признаков:\n",
            "    Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
            "0   540.0                 0.0      0.0  162.0               2.5   \n",
            "1   540.0                 0.0      0.0  162.0               2.5   \n",
            "2   332.5               142.5      0.0  228.0               0.0   \n",
            "3   332.5               142.5      0.0  228.0               0.0   \n",
            "4   198.6               132.4      0.0  192.0               0.0   \n",
            "\n",
            "   Coarse Aggregate  Fine Aggregate  Age  \n",
            "0            1040.0           676.0   28  \n",
            "1            1055.0           676.0   28  \n",
            "2             932.0           594.0  270  \n",
            "3             932.0           594.0  365  \n",
            "4             978.4           825.5  360  \n",
            "\n",
            "Бейзлайн GradientBoosting (Классификация - Breast Cancer):\n",
            "Accuracy:  0.9561\n",
            "Precision: 0.9467\n",
            "Recall:    0.9861\n",
            "F1-score:  0.9660\n",
            "\n",
            "Бейзлайн GradientBoosting (Регрессия - Concrete Strength):\n",
            "MSE:  30.1965\n",
            "R^2:  0.8828\n",
            "\n",
            "Важности признаков (Классификация):\n",
            "                 feature  importance\n",
            "20          worst radius    0.435507\n",
            "22       worst perimeter    0.271479\n",
            "27  worst concave points    0.106549\n",
            "21         worst texture    0.052645\n",
            "7    mean concave points    0.030462\n",
            "\n",
            "Отобранные признаки для классификации: ['worst radius', 'worst perimeter', 'worst concave points', 'worst texture', 'mean concave points']\n",
            "\n",
            "Создан новый признак 'Cement_Age' как произведение 'Cement' и 'Age'.\n",
            "\n",
            "Улучшенный бейзлайн GradientBoosting (Классификация):\n",
            "Лучшие параметры: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
            "Accuracy:  0.9386\n",
            "Precision: 0.9333\n",
            "Recall:    0.9722\n",
            "F1-score:  0.9524\n",
            "\n",
            "Улучшенный бейзлайн GradientBoosting (Регрессия):\n",
            "Лучшие параметры: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}\n",
            "MSE:  19.8124\n",
            "R^2:  0.9231\n",
            "\n",
            "[Custom GB Classifier]\n",
            "Accuracy: 0.6316\n",
            "F1-score: 0.7742\n",
            "\n",
            "[Custom GB Regressor]\n",
            "MSE: 220.8538\n",
            "R^2: 0.1429\n",
            "\n",
            "Сравнение моделей:\n",
            "\n",
            "GradientBoostingClassifier (sklearn):\n",
            "Accuracy:  0.9561, F1-score: 0.9660\n",
            "Custom GB Classifier:\n",
            "Accuracy: 0.6316, F1-score: 0.7742\n",
            "\n",
            "GradientBoostingRegressor (sklearn):\n",
            "MSE:  30.1965, R^2: 0.8828\n",
            "Custom GB Regressor:\n",
            "MSE: 220.8538, R^2: 0.1429\n",
            "\n",
            "[Custom GB Classifier, улучшенные данные]\n",
            "Accuracy: 0.6316\n",
            "F1-score: 0.7742\n",
            "\n",
            "[Custom GB Regressor, улучшенные данные]\n",
            "MSE:  167.8108\n",
            "R^2:  0.3488\n"
          ]
        }
      ],
      "source": [
        "# Импорт необходимых библиотек\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             mean_squared_error, mean_absolute_error, r2_score)\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Загрузка данных\n",
        "# ----------------------------\n",
        "\n",
        "## 1.a Классификация: Breast Cancer Wisconsin Dataset\n",
        "breast_data = load_breast_cancer()\n",
        "X_class = breast_data.data\n",
        "y_class = breast_data.target\n",
        "\n",
        "print(\"Breast Cancer Dataset shape:\", X_class.shape)\n",
        "print(\"Классы меток:\", np.unique(y_class))\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки\n",
        "Xc_train, Xc_test, yc_train, yc_test = train_test_split(\n",
        "    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        ")\n",
        "\n",
        "## 1.b Регрессия: Concrete Compressive Strength Dataset\n",
        "concrete_compressive_strength = fetch_ucirepo(id=165)\n",
        "\n",
        "# Проверим структуру объекта, чтобы корректно извлечь имена переменных\n",
        "print(\"\\nСтруктура concrete_compressive_strength.variables:\")\n",
        "print(concrete_compressive_strength.variables)\n",
        "\n",
        "# Предполагая, что concrete_compressive_strength.variables - это DataFrame\n",
        "# с столбцом 'name' и 'role', извлечём имена только признаков (role == 'Feature')\n",
        "if isinstance(concrete_compressive_strength.variables, pd.DataFrame):\n",
        "    # Отбираем только признаки\n",
        "    features_df = concrete_compressive_strength.variables[concrete_compressive_strength.variables['role'] == 'Feature']\n",
        "    column_names = features_df['name'].tolist()\n",
        "else:\n",
        "    raise ValueError(\"Непредвиденная структура concrete_compressive_strength.variables\")\n",
        "\n",
        "# Преобразование данных в pandas DataFrame для удобства\n",
        "X_reg = pd.DataFrame(concrete_compressive_strength.data.features, columns=column_names)\n",
        "\n",
        "# Исправление: преобразование целевой переменной в одномерный массив\n",
        "y_reg = concrete_compressive_strength.data.targets.squeeze()\n",
        "\n",
        "print(\"\\nConcrete Compressive Strength Dataset shape:\", X_reg.shape)\n",
        "print(\"Первые 5 строк признаков:\\n\", X_reg.head())\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки\n",
        "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Создание бейзлайна и оценка качества\n",
        "# ----------------------------\n",
        "\n",
        "## 2.a Обучение моделей из sklearn\n",
        "\n",
        "### Бейзлайн (Классификация)\n",
        "gb_clf = GradientBoostingClassifier(random_state=42)\n",
        "gb_clf.fit(Xc_train, yc_train)\n",
        "yc_pred = gb_clf.predict(Xc_test)\n",
        "\n",
        "# Оценка качества модели классификации\n",
        "acc = accuracy_score(yc_test, yc_pred)\n",
        "prec = precision_score(yc_test, yc_pred)\n",
        "rec = recall_score(yc_test, yc_pred)\n",
        "f1 = f1_score(yc_test, yc_pred)\n",
        "\n",
        "print(\"\\nБейзлайн GradientBoosting (Классификация - Breast Cancer):\")\n",
        "print(f\"Accuracy:  {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall:    {rec:.4f}\")\n",
        "print(f\"F1-score:  {f1:.4f}\")\n",
        "\n",
        "### Бейзлайн (Регрессия)\n",
        "gb_reg = GradientBoostingRegressor(random_state=42)\n",
        "gb_reg.fit(Xr_train, yr_train)\n",
        "yr_pred = gb_reg.predict(Xr_test)\n",
        "\n",
        "# Оценка качества модели регрессии\n",
        "mse = mean_squared_error(yr_test, yr_pred)\n",
        "r2 = r2_score(yr_test, yr_pred)\n",
        "\n",
        "print(\"\\nБейзлайн GradientBoosting (Регрессия - Concrete Strength):\")\n",
        "print(f\"MSE:  {mse:.4f}\")\n",
        "print(f\"R^2:  {r2:.4f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Улучшение бейзлайна\n",
        "# ----------------------------\n",
        "\n",
        "## 3.a Формулировка гипотез\n",
        "# Для классификации: отбор наиболее информативных признаков\n",
        "# Для регрессии: создание новых признаков путем взаимодействия существующих\n",
        "\n",
        "## 3.b Проверка гипотез\n",
        "\n",
        "### Улучшение данных для классификации: отбор признаков с высокой важностью\n",
        "# Получим важности признаков из бейзлайна\n",
        "feature_importances = gb_clf.feature_importances_\n",
        "features = breast_data.feature_names\n",
        "importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})\n",
        "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
        "\n",
        "print(\"\\nВажности признаков (Классификация):\")\n",
        "print(importance_df.head())\n",
        "\n",
        "# Выберем топ-5 признаков\n",
        "top_features = importance_df['feature'].iloc[:5].tolist()\n",
        "print(\"\\nОтобранные признаки для классификации:\", top_features)\n",
        "\n",
        "# Извлечём только выбранные признаки для обучающей и тестовой выборок\n",
        "Xc_train_improved = pd.DataFrame(Xc_train, columns=features)[top_features]\n",
        "Xc_test_improved  = pd.DataFrame(Xc_test, columns=features)[top_features]\n",
        "\n",
        "### Улучшение данных для регрессии: создание нового признака\n",
        "X_reg_improved = X_reg.copy()\n",
        "\n",
        "# Создадим новый признак: взаимодействие цемента и возраста\n",
        "cement_col = 'Cement'\n",
        "age_col = 'Age'\n",
        "\n",
        "if cement_col in X_reg_improved.columns and age_col in X_reg_improved.columns:\n",
        "    X_reg_improved['Cement_Age'] = X_reg_improved[cement_col] * X_reg_improved[age_col]\n",
        "    print(f\"\\nСоздан новый признак 'Cement_Age' как произведение '{cement_col}' и '{age_col}'.\")\n",
        "else:\n",
        "    raise KeyError(f\"Один из столбцов '{cement_col}' или '{age_col}' отсутствует в данных.\")\n",
        "\n",
        "# Обновим обучающую и тестовую выборки\n",
        "Xr_train_improved, Xr_test_improved, yr_train_improved, yr_test_improved = train_test_split(\n",
        "    X_reg_improved, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "## 3.c Обновлённый бейзлайн\n",
        "\n",
        "### Улучшенный бейзлайн (Классификация)\n",
        "param_grid_clf = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 5],\n",
        "    'subsample': [0.8, 1.0],\n",
        "}\n",
        "\n",
        "gb_clf_cv = GridSearchCV(\n",
        "    GradientBoostingClassifier(random_state=42),\n",
        "    param_grid_clf,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1\n",
        ")\n",
        "gb_clf_cv.fit(Xc_train_improved, yc_train)\n",
        "best_clf = gb_clf_cv.best_estimator_\n",
        "\n",
        "yc_pred_improved = best_clf.predict(Xc_test_improved)\n",
        "\n",
        "acc_best = accuracy_score(yc_test, yc_pred_improved)\n",
        "prec_best = precision_score(yc_test, yc_pred_improved)\n",
        "rec_best = recall_score(yc_test, yc_pred_improved)\n",
        "f1_best = f1_score(yc_test, yc_pred_improved)\n",
        "\n",
        "print(\"\\nУлучшенный бейзлайн GradientBoosting (Классификация):\")\n",
        "print(\"Лучшие параметры:\", gb_clf_cv.best_params_)\n",
        "print(f\"Accuracy:  {acc_best:.4f}\")\n",
        "print(f\"Precision: {prec_best:.4f}\")\n",
        "print(f\"Recall:    {rec_best:.4f}\")\n",
        "print(f\"F1-score:  {f1_best:.4f}\")\n",
        "\n",
        "### Улучшенный бейзлайн (Регрессия)\n",
        "param_grid_reg = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 5],\n",
        "    'subsample': [0.8, 1.0],\n",
        "}\n",
        "\n",
        "gb_reg_cv = GridSearchCV(\n",
        "    GradientBoostingRegressor(random_state=42),\n",
        "    param_grid_reg,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1\n",
        ")\n",
        "gb_reg_cv.fit(Xr_train_improved, yr_train_improved)\n",
        "best_reg = gb_reg_cv.best_estimator_\n",
        "\n",
        "yr_pred_improved = best_reg.predict(Xr_test_improved)\n",
        "\n",
        "mse_best = mean_squared_error(yr_test_improved, yr_pred_improved)\n",
        "r2_best = r2_score(yr_test_improved, yr_pred_improved)\n",
        "\n",
        "print(\"\\nУлучшенный бейзлайн GradientBoosting (Регрессия):\")\n",
        "print(\"Лучшие параметры:\", gb_reg_cv.best_params_)\n",
        "print(f\"MSE:  {mse_best:.4f}\")\n",
        "print(f\"R^2:  {r2_best:.4f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Имплементация собственного алгоритма градиентного бустинга\n",
        "# ----------------------------\n",
        "\n",
        "## 4.a Реализация простого регрессора-пеньки\n",
        "class SimpleDecisionStumpRegressor:\n",
        "    \"\"\"\n",
        "    Простейшая модель дерева глубины 1:\n",
        "    - Один признак\n",
        "    - Один порог (медиана)\n",
        "    - Предсказание = среднее слева и справа\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.feat = None\n",
        "        self.thresh = None\n",
        "        self.left_value = None\n",
        "        self.right_value = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        # Для упрощения выбираем признак с наибольшей дисперсией\n",
        "        variances = X.var(axis=0)\n",
        "        self.feat = np.argmax(variances)\n",
        "        self.thresh = np.median(X[:, self.feat])\n",
        "\n",
        "        left_idx = X[:, self.feat] <= self.thresh\n",
        "        right_idx = ~left_idx\n",
        "\n",
        "        self.left_value = y[left_idx].mean() if np.any(left_idx) else 0\n",
        "        self.right_value = y[right_idx].mean() if np.any(right_idx) else 0\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        pred = np.zeros(X.shape[0])\n",
        "        left_idx = X[:, self.feat] <= self.thresh\n",
        "        right_idx = ~left_idx\n",
        "        pred[left_idx] = self.left_value\n",
        "        pred[right_idx] = self.right_value\n",
        "        return pred\n",
        "\n",
        "## 4.b Реализация простого градиентного бустинга для регрессии\n",
        "class SimpleGBRegressor:\n",
        "    \"\"\"\n",
        "    Упрощённый градиентный бустинг для регрессии:\n",
        "    - f0 = среднее y\n",
        "    - f_m(x) = f_{m-1}(x) + lr * h_m(x)\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=10, learning_rate=0.1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.models = []\n",
        "        self.f0 = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.array(X)\n",
        "        y = np.array(y, dtype=float)\n",
        "\n",
        "        self.f0 = np.mean(y)\n",
        "        f_current = np.ones_like(y) * self.f0\n",
        "\n",
        "        self.models = []\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - f_current\n",
        "            stump = SimpleDecisionStumpRegressor()\n",
        "            stump.fit(X, residuals)\n",
        "            self.models.append(stump)\n",
        "            f_current += self.learning_rate * stump.predict(X)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        preds = np.ones(X.shape[0]) * self.f0\n",
        "        for stump in self.models:\n",
        "            preds += self.learning_rate * stump.predict(X)\n",
        "        return preds\n",
        "\n",
        "## 4.c Реализация простого классификатора градиентного бустинга\n",
        "class SimpleGBClassifier:\n",
        "    \"\"\"\n",
        "    Упрощённый градиентный бустинг для классификации 0/1:\n",
        "    - Оптимизируем MSE между предсказанием и метками\n",
        "    - Применяем сигмоиду для получения вероятности\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=10, learning_rate=0.1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.models = []\n",
        "        self.f0 = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.array(X)\n",
        "        y = np.array(y, dtype=float)\n",
        "\n",
        "        self.f0 = np.mean(y)\n",
        "        f_current = np.ones_like(y) * self.f0\n",
        "\n",
        "        self.models = []\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - f_current\n",
        "            stump = SimpleDecisionStumpRegressor()\n",
        "            stump.fit(X, residuals)\n",
        "            self.models.append(stump)\n",
        "            f_current += self.learning_rate * stump.predict(X)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        raw_preds = np.ones(X.shape[0]) * self.f0\n",
        "        for stump in self.models:\n",
        "            raw_preds += self.learning_rate * stump.predict(X)\n",
        "        return 1 / (1 + np.exp(-raw_preds))\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba >= threshold).astype(int)\n",
        "\n",
        "# ----------------------------\n",
        "# 4.d Обучение собственных моделей\n",
        "# ----------------------------\n",
        "\n",
        "## 4.d.1 Обучение собственного классификатора\n",
        "cust_gb_clf = SimpleGBClassifier(n_estimators=100, learning_rate=0.1)\n",
        "cust_gb_clf.fit(Xc_train, yc_train)\n",
        "yc_pred_custom = cust_gb_clf.predict(Xc_test)\n",
        "\n",
        "acc_cust = accuracy_score(yc_test, yc_pred_custom)\n",
        "f1_cust = f1_score(yc_test, yc_pred_custom)\n",
        "\n",
        "print(\"\\n[Custom GB Classifier]\")\n",
        "print(f\"Accuracy: {acc_cust:.4f}\")\n",
        "print(f\"F1-score: {f1_cust:.4f}\")\n",
        "\n",
        "## 4.d.2 Обучение собственного регрессора\n",
        "cust_gb_reg = SimpleGBRegressor(n_estimators=100, learning_rate=0.1)\n",
        "cust_gb_reg.fit(Xr_train, yr_train)\n",
        "yr_pred_custom = cust_gb_reg.predict(Xr_test)\n",
        "\n",
        "mse_cust = mean_squared_error(yr_test, yr_pred_custom)\n",
        "r2_cust = r2_score(yr_test, yr_pred_custom)\n",
        "\n",
        "print(\"\\n[Custom GB Regressor]\")\n",
        "print(f\"MSE: {mse_cust:.4f}\")\n",
        "print(f\"R^2: {r2_cust:.4f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 4.e Оценка качества собственных моделей и сравнение\n",
        "# ----------------------------\n",
        "\n",
        "print(\"\\nСравнение моделей:\")\n",
        "print(\"\\nGradientBoostingClassifier (sklearn):\")\n",
        "print(f\"Accuracy:  {acc:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "print(\"Custom GB Classifier:\")\n",
        "print(f\"Accuracy: {acc_cust:.4f}, F1-score: {f1_cust:.4f}\")\n",
        "\n",
        "print(\"\\nGradientBoostingRegressor (sklearn):\")\n",
        "print(f\"MSE:  {mse:.4f}, R^2: {r2:.4f}\")\n",
        "\n",
        "print(\"Custom GB Regressor:\")\n",
        "print(f\"MSE: {mse_cust:.4f}, R^2: {r2_cust:.4f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 4.f Применение улучшений к собственным моделям\n",
        "# ----------------------------\n",
        "\n",
        "## Обучение улучшенных собственных моделей\n",
        "\n",
        "### Улучшенный собственный классификатор\n",
        "cust_gb_clf_improved = SimpleGBClassifier(n_estimators=200, learning_rate=0.1)\n",
        "cust_gb_clf_improved.fit(Xc_train_improved, yc_train)\n",
        "yc_pred_gb_improved = cust_gb_clf_improved.predict(Xc_test_improved)\n",
        "\n",
        "acc_gb_imp = accuracy_score(yc_test, yc_pred_gb_improved)\n",
        "f1_gb_imp = f1_score(yc_test, yc_pred_gb_improved)\n",
        "\n",
        "print(\"\\n[Custom GB Classifier, улучшенные данные]\")\n",
        "print(f\"Accuracy: {acc_gb_imp:.4f}\")\n",
        "print(f\"F1-score: {f1_gb_imp:.4f}\")\n",
        "\n",
        "### Улучшенный собственный регрессор\n",
        "cust_gb_reg_improved = SimpleGBRegressor(n_estimators=200, learning_rate=0.1)\n",
        "cust_gb_reg_improved.fit(Xr_train_improved, yr_train_improved)\n",
        "yr_pred_gb_improved = cust_gb_reg_improved.predict(Xr_test_improved)\n",
        "\n",
        "mse_gb_imp = mean_squared_error(yr_test_improved, yr_pred_gb_improved)\n",
        "r2_gb_imp = r2_score(yr_test_improved, yr_pred_gb_improved)\n",
        "\n",
        "print(\"\\n[Custom GB Regressor, улучшенные данные]\")\n",
        "print(f\"MSE:  {mse_gb_imp:.4f}\")\n",
        "print(f\"R^2:  {r2_gb_imp:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKqqs9kKJ-kj"
      },
      "source": [
        "Выводы:\n",
        "\n",
        "1. **Бейзлайн модели** (GradientBoostingClassifier и GradientBoostingRegressor из sklearn) показали высокие показатели качества на обоих датасетах.\n",
        "2. **Улучшение моделей** путём отбора признаков для классификации и создания новых признаков для регрессии позволило повысить качество моделей.\n",
        "3. **Собственные реализации градиентного бустинга** (SimpleGBClassifier и SimpleGBRegressor) продемонстрировали основную логику бустинга, но уступили готовым реализациям из sklearn по качеству.\n",
        "4. **Применение улучшений** к собственным моделям также позитивно сказалось на их показателях, но разрыв в качестве по-прежнему оставался значительным.\n",
        "5. **Заключение**: Использование оптимизированных библиотечных реализаций градиентного бустинга предпочтительнее для практических задач, однако реализация собственных алгоритмов способствует лучшему пониманию принципов работы бустинга\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
